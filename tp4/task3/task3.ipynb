{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff23ed64-24bc-4a24-888e-b0c4900f53cb",
   "metadata": {},
   "source": [
    "# Task 3: New Data Pipeline Using Delta Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1a8e9",
   "metadata": {},
   "source": [
    "Gabriel Lima Barros - 2020006531\n",
    "\n",
    "Maria Luiza Le√£o Silva - 2020100953"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5813d-a2be-4c3a-bf31-35477eea73d0",
   "metadata": {},
   "source": [
    "## Creating Delta Format Datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54deffc6-5cb1-4262-976c-e023483eac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/gabrielbarros/.ivy2/cache\n",
      "The jars for the packages stored in: /home/gabrielbarros/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ad2bc82a-fb7d-4d6e-a291-f560630c2deb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 326ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ad2bc82a-fb7d-4d6e-a291-f560630c2deb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/02/05 23:11:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/05 23:11:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/05 23:11:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/02/05 23:11:27 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/02/05 23:11:27 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/02/05 23:11:27 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/02/05 23:11:27 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/02/05 23:11:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/02/05 23:11:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/05 23:11:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/05 23:11:58 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/05 23:12:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:12:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:12:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:12:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:12:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:12:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:12:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:12:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:12:17 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/05 23:12:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "Processing time for delta data lake: 76.44 seconds                              \n"
     ]
    }
   ],
   "source": [
    "! python3 /home/gabrielbarros/tp4/task3/create_delta_lake.py delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23235d8-3445-4803-8d84-0c5f26e4afc2",
   "metadata": {},
   "source": [
    "## Ingest V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c466437f-0439-4b7c-a6f7-4da70894b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/gabrielbarros/.ivy2/cache\n",
      "The jars for the packages stored in: /home/gabrielbarros/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5521bd06-f926-4f5d-83c9-01dce9f32084;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 178ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5521bd06-f926-4f5d-83c9-01dce9f32084\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/02/05 23:12:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/05 23:12:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/05 23:12:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/02/05 23:12:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/02/05 23:12:44 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/02/05 23:12:44 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/02/05 23:12:44 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/02/05 23:12:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/02/05 23:13:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "Silver Layer Update Time: 29.565 seconds                                        \n",
      "Gold Layer Update Time: 6.840 seconds\n",
      "Bronze layer storage: 44.65 mb\n",
      "Silver layer storage: 143.24 mb\n",
      "Gold layer storage: 17.9 mb\n",
      "Total storage: 205.78 mb\n"
     ]
    }
   ],
   "source": [
    "! python3 /home/gabrielbarros/tp4/task3/ingest_v_delta.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c0705-11d1-4d94-bd16-509526c11e10",
   "metadata": {},
   "source": [
    "### Parquet x Delta Ingest V2 time comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea329f-44c4-4379-bd7c-f652de68698b",
   "metadata": {},
   "source": [
    "| Format  | Silver Layer (s) | Gold Layer (s) | Total Time (s) |\n",
    "|---------|-----------------|----------------|----------------|\n",
    "| Parquet | 25.89           | 10.29          | 49.45          |\n",
    "| Delta   | 29.56           | 06.84          | 36.40          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6450566-e591-4331-a8b2-526c802a4cf0",
   "metadata": {},
   "source": [
    "Delta vs. Parquet\n",
    "\n",
    "Silver Layer Update:\n",
    "\n",
    "* Delta Lake: 29.56 seconds\n",
    "* Parquet: 25.89 seconds\n",
    "* Observation: Delta Lake took slightly longer due to its transactional overhead and the use of the MERGE statement, which ensures data consistency.\n",
    "\n",
    "Gold Layer Update:\n",
    "\n",
    "* Delta Lake: 6.84 seconds\n",
    "* Parquet: 10.29 seconds\n",
    "* Observation: Delta Lake was faster because its optimized storage and indexing reduced read and write latencies.\n",
    "\n",
    "Total Time:\n",
    "\n",
    "* Delta Lake: 36.40 seconds\n",
    "* Parquet: 49.45 seconds\n",
    "* Observation: While Delta Lake's write operations had additional overhead in the Silver layer, the optimized reads for the Gold layer resulted in a significantly faster total runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564529bb-2ba9-4ed8-8c76-3e0d2bd12350",
   "metadata": {},
   "source": [
    "### Parquet x Delta space comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba7501-06c2-45dc-bc35-56e85c057e60",
   "metadata": {},
   "source": [
    "| Format  | Bronze Layer (MB) | Silver Layer (MB) | Gold Layer (MB) | Total Storage (MB) |\n",
    "|---------|------------------|------------------|----------------|-----------------|\n",
    "| Parquet | 44.64            | 124.02           | 24.74          | 193.41          |\n",
    "| Delta   | 44.65            | 143.25           | 17.09          | 205.78          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec3eb2e-7b0d-446b-ac80-a18ff0a04b18",
   "metadata": {},
   "source": [
    "Delta vs. Parquet\n",
    "\n",
    "Bronze Layer:\n",
    "\n",
    "* Delta Lake: 44.65 MB\n",
    "* Parquet: 44.64 MB\n",
    "* Observation: Minimal difference as both formats store raw data.\n",
    "\n",
    "Silver Layer:\n",
    "\n",
    "* Delta Lake: 143.25 MB\n",
    "* Parquet: 124.02 MB\n",
    "* Observation: Delta Lake requires additional storage for metadata and transaction logs, increasing the Silver layer size.\n",
    "\n",
    "Gold Layer:\n",
    "\n",
    "* Delta Lake: 17.09 MB\n",
    "* Parquet: 24.74 MB\n",
    "* Observation: Delta Lake's optimized file compaction reduced the storage required for aggregated data.\n",
    "\n",
    "Total Storage:\n",
    "\n",
    "* Delta Lake: 205.78 MB\n",
    "* Parquet: 193.41 MB\n",
    "* Observation: Delta Lake uses slightly more storage overall due to transaction logs and metadata but compensates with faster reads and write operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1fd55-604f-40cd-bedb-ea63e12d1d33",
   "metadata": {},
   "source": [
    "## Update playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29e3d331-a3d5-494d-bc81-d23fa80dedb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/gabrielbarros/.ivy2/cache\n",
      "The jars for the packages stored in: /home/gabrielbarros/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c28641bd-60bb-46a3-af5d-4d4e89d602c7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 271ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c28641bd-60bb-46a3-af5d-4d4e89d602c7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/13ms)\n",
      "25/02/05 23:13:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/05 23:13:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/05 23:13:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/02/05 23:13:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/02/05 23:13:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/02/05 23:13:35 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/02/05 23:13:35 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "[INFO] Updating datalake/silver Layer for playlist 11992...\n",
      "25/02/05 23:13:43 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[INFO] datalake/silver Layer updated in 15.42 seconds.                          \n",
      "[INFO] Updating datalake/gold Layer for playlist 11992...\n",
      "[INFO] datalake/gold Layer updated in 4.74 seconds.                             \n",
      "[INFO] Playlist 11992 update completed in 25.12 seconds.\n"
     ]
    }
   ],
   "source": [
    "! python3 /home/gabrielbarros/tp4/task3/update_playlist_delta.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7116050-05b5-493e-9511-82a30fbcded4",
   "metadata": {},
   "source": [
    "# Parquet x Delta playlist update time comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ce3f6-152c-427d-bba2-3bf09df453ca",
   "metadata": {},
   "source": [
    "| Format  | Silver Layer (s) | Gold Layer (s) | Total Time (s) |\n",
    "|---------|-----------------|----------------|----------------|\n",
    "| Parquet | 13.67           | 05.56          | 28.58          |\n",
    "| Delta   | 15.42           | 04.74          | 25.12          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3f2b5-f0e0-4b11-9030-949563ec29ef",
   "metadata": {},
   "source": [
    "Performance Observations\n",
    "\n",
    "Silver Layer Update:\n",
    "\n",
    "* Parquet: 13.67 seconds\n",
    "* Delta Lake: 15.42 seconds\n",
    "* Observation: Delta Lake takes slightly longer in the Silver Layer due to its transactional overhead (e.g., managing metadata and maintaining ACID properties).\n",
    "\n",
    "Gold Layer Update:\n",
    "\n",
    "* Parquet: 5.56 seconds\n",
    "* Delta Lake: 4.74 seconds\n",
    "* Observation: Delta Lake outperforms Parquet in the Gold Layer due to its optimized file structure and indexing.\n",
    "\n",
    "Total Time:\n",
    "\n",
    "* Parquet: 28.58 seconds\n",
    "* Delta Lake: 25.12 seconds\n",
    "* Observation: Delta Lake completes the entire playlist update pipeline faster by ~12%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c4d66-3a0d-4257-86d6-d2a6a048cb18",
   "metadata": {},
   "source": [
    "## Ingest V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91386a37-27fd-4de2-967d-58f0374e5ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/gabrielbarros/.ivy2/cache\n",
      "The jars for the packages stored in: /home/gabrielbarros/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7b7baaed-d086-44f2-9894-e5986ed6792d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 410ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7b7baaed-d086-44f2-9894-e5986ed6792d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n",
      "25/02/05 23:14:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/05 23:14:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/05 23:14:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/02/05 23:14:02 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/02/05 23:14:02 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/02/05 23:14:02 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/02/05 23:14:02 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/02/05 23:14:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/02/05 23:14:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:14:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/05 23:14:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "Silver Layer Update Time: 26.061 seconds                                        \n",
      "Gold Layer Update Time: 5.930 seconds\n",
      "Bronze layer storage: 44.65 mb\n",
      "Silver layer storage: 230.88 mb\n",
      "Gold layer storage: 19.94 mb\n",
      "Total storage: 295.47 mb\n"
     ]
    }
   ],
   "source": [
    "! python3 /home/gabrielbarros/tp4/task3/ingest_v_delta.py -p \"/shared/sampled/playlists_v3.json\" -t \"/shared/sampled/tracks_v3.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cdd4a5-5e78-47b9-8c7c-d2b720bb431c",
   "metadata": {},
   "source": [
    "### Parquet x Delta Ingest V3 time comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282229f-3cf0-4f6a-b510-9b574a4688db",
   "metadata": {},
   "source": [
    "| Format  | Silver Layer (s) | Gold Layer (s) | Total Time (s) |\n",
    "|---------|-----------------|----------------|----------------|\n",
    "| Parquet | 15.03           | 07.55          | 31.00          |\n",
    "| Delta   | 26.06           | 05.93          | 31.99          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc8a70-70fc-4e8a-84b2-192bb3a8d64b",
   "metadata": {},
   "source": [
    "Silver Layer Update:\n",
    "\n",
    "* Parquet: 15.03 seconds\n",
    "* Delta Lake: 26.06 seconds\n",
    "* Observation: Delta Lake takes longer due to overhead from transactional consistency and metadata management, which ensures reliable and accurate updates.\n",
    "\n",
    "Gold Layer Update:\n",
    "\n",
    "* Parquet: 7.55 seconds\n",
    "* Delta Lake: 5.93 seconds\n",
    "* Observation: Delta Lake outperforms Parquet in the Gold Layer due to optimized querying and file compaction strategies.\n",
    "\n",
    "Total Time:\n",
    "\n",
    "* Parquet: 31.00 seconds\n",
    "* Delta Lake: 31.99 seconds\n",
    "* Observation: The overall runtime difference is negligible (~3%), with Delta Lake slightly slower due to Silver Layer overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c75a45-8d65-4343-8067-b2594ba4585a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
